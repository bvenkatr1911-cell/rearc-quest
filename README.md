# Rearc Data Quest â€“ End-to-End Data Pipeline

This repository contains my full implementation of the Rearc Data Quest, including S3 ingestion, API sourcing, data analytics, and an automated AWS serverless pipeline deployed using Terraform.

---

## ğŸ“Œ Project Structure

rearc-data-quest/
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ part1_sync_bls.py
â”‚ â”œâ”€â”€ part2_fetch_population.py
â”‚ â””â”€â”€ rearc_data_quest_part3.ipynb
â”‚
â”œâ”€â”€ lambda_ingest/ # Lambda that runs Part 1 + Part 2
â”‚ â”œâ”€â”€ main.py
â”‚ â”œâ”€â”€ part1_sync_bls.py
â”‚ â”œâ”€â”€ part2_fetch_population.py
â”‚ â””â”€â”€ (third-party dependencies)
â”‚
â”œâ”€â”€ lambda_analytics/ # Lambda that runs Part 3 queries
â”‚ â””â”€â”€ main.py
â”‚
â”œâ”€â”€ terraform/
â”‚ â”œâ”€â”€ main.tf
â”‚ â”œâ”€â”€ terraform.tfvars
â”‚ â””â”€â”€ .terraform.lock.hcl
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md



---

## ğŸ§© Part 1 â€” BLS Data â†’ S3 Sync

**Goal:**  
Fetch all `.txt` time-series files from the BLS Productivity dataset and keep them synchronized in S3.

**Highlights:**
- Automatically discovers all files (no hard-coding)
- Downloads only new/changed files
- Pushes to S3 bucket  
- Handles 403 errors by using a valid `User-Agent`
- Scripts reused inside Lambda

**Code:**  
`src/part1_sync_bls.py`

---

## ğŸ§© Part 2 â€” Population API â†’ S3

**Goal:**  
Fetch U.S. population data from:
https://honolulu-api.datausa.io/tesseract/data.jsonrecords?cube=acs_yg_total_population_1&drilldowns=Year,Nation&measures=Population


**Highlights:**
- Stores *all* years available (not only 2013â€“2018)
- Normalizes JSON into simple `{year, population}` rows
- Saves result in S3 at:  
  `rearc-data-quest/population/us_population_all_years.json`

**Code:**  
`src/part2_fetch_population.py`

---

## ğŸ§© Part 3 â€” Analytics (Notebook)

**Goal:**  
Use Pandas to answer three analytical questions:

### 1ï¸âƒ£ Population Mean & Std Dev (2013â€“2018)
Computed from the API dataset.

### 2ï¸âƒ£ Best Year for Each Series
For every `series_id`:
- Sum Q01â€“Q04 for each year
- Pick the year with the highest total

Resulting dataframe includes:
series_id | year | year_sum


### 3ï¸âƒ£ Population + Value Join  
For:
series_id = PRS30006032 and period = Q01


Joined with population data for the matching year.

**Notebook:**  
`src/rearc_data_quest_part3.ipynb`

---

## ğŸ§© Part 4 â€” Automated Pipeline (Terraform)

This is the full AWS pipeline orchestrating the quest.

### âœ… Resources created
- **S3 Bucket** (pre-existing, injected via variables)
- **Lambda #1: Ingest Function**
  - Runs Part 1 and Part 2
  - Scheduled daily via EventBridge
- **SQS Queue**
  - Triggered when new population JSON is uploaded
- **Lambda #2: Analytics Function**
  - Reads BLS + population data
  - Executes analytics from Part 3
  - Logs results to CloudWatch

### ğŸ”§ Deployment
Inside the `terraform/` directory:

```bash
terraform init
terraform plan
terraform apply

Terraform code:
terraform/main.tf

ğŸ“¤ Deliverables Submitted
Part	Deliverable
Part 1	Python script + S3 dataset
Part 2	Python script + S3 JSON
Part 3	Jupyter Notebook (.ipynb) with queries & results
Part 4	Terraform Infrastructure as Code

ğŸ“ Notes on AI Usage

I used AI as a helper for:
Debugging Python
Understanding AWS/Terraform syntax
Structuring the project
All final code was tested, fixed, and validated manually.
